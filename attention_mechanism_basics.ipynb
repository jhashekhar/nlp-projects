{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attention_mechanism_basics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMVgJK3x+l1lTIiUIt9atcS"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M1LBUGOGzQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5j0MxXLQFi4",
        "colab_type": "code",
        "outputId": "6f3626ce-98ac-48b3-9d41-529b202591d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# vector of embeddings\n",
        "# let's say the number of the tokens/words or for the sake of simplicity the length of sentence is V = 3\n",
        "# with each V_i (embedding) be of size (1 x 2)\n",
        "# so [1, 6, 10]\n",
        "\n",
        "torch.Tensor([[0.2, 0.3], [0.3, 0.4], [0.7, 0.5]]).unsqueeze(0).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPOvNEhpHPNY",
        "colab_type": "code",
        "outputId": "e03e9e51-0e9c-4f4a-9526-3bb939205dc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "torch.manual_seed(33)\n",
        "q = torch.randn(1, 3, 2)\n",
        "k = torch.randn(1, 3, 2)\n",
        "print(f\"Query: \\n{q} \\n\\n Key: \\n{k}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Query: \n",
            "tensor([[[ 1.1982, -0.3998],\n",
            "         [-0.3476, -0.2759],\n",
            "         [-2.3094, -1.0931]]]) \n",
            "\n",
            " Key: \n",
            "tensor([[[-0.0808,  0.7721],\n",
            "         [-1.1370, -0.4773],\n",
            "         [-1.0679,  1.0688]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BVKTa-aJBcd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q, k = q.reshape(1, -1), k.view(1, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErNvS-xDT9_l",
        "colab_type": "code",
        "outputId": "ffdabd79-d9bc-4dca-8ad0-381a07f2ae05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(q.size(), k.size())\n",
        "q.view(-1, 1).size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 6]) torch.Size([1, 6])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XSpGmvtLTGF",
        "colab_type": "code",
        "outputId": "c3f90760-6dd6-4e2f-9635-118b96cfe4a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "torch.matmul(q.view(-1, 1), k)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0968,  0.9251, -1.3624, -0.5718, -1.2796,  1.2806],\n",
              "        [ 0.0323, -0.3087,  0.4546,  0.1908,  0.4270, -0.4273],\n",
              "        [ 0.0281, -0.2684,  0.3953,  0.1659,  0.3712, -0.3715],\n",
              "        [ 0.0223, -0.2130,  0.3137,  0.1317,  0.2946, -0.2948],\n",
              "        [ 0.1866, -1.7830,  2.6259,  1.1022,  2.4662, -2.4682],\n",
              "        [ 0.0883, -0.8440,  1.2430,  0.5217,  1.1674, -1.1683]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvMbiHjUIp18",
        "colab_type": "code",
        "outputId": "296b1ba7-4c9e-4dfc-a9be-29f4731ac94d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Softmax(similarity_measure)\n",
        "softmax = torch.nn.Softmax(dim=0)\n",
        "softmax(torch.matmul(q.view(-1, 1), k))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1443, 0.4647, 0.0117, 0.0642, 0.0142, 0.5918],\n",
              "        [0.1642, 0.1353, 0.0717, 0.1377, 0.0782, 0.1073],\n",
              "        [0.1635, 0.1409, 0.0676, 0.1343, 0.0740, 0.1134],\n",
              "        [0.1626, 0.1489, 0.0623, 0.1298, 0.0685, 0.1225],\n",
              "        [0.1916, 0.0310, 0.6290, 0.3425, 0.6011, 0.0139],\n",
              "        [0.1737, 0.0792, 0.1578, 0.1917, 0.1640, 0.0511]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0jLyM_mXnwI",
        "colab_type": "code",
        "outputId": "9049f5e2-f9ca-4078-810d-bcdad376e330",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Multihead Attention Layer\n",
        "embedding_size = 6\n",
        "num_heads = 3\n",
        "dropout = 0.5\n",
        "sequence_length = 10\n",
        "batch_size = 4\n",
        "\n",
        "\n",
        "\n",
        "self_attention = torch.nn.MultiheadAttention(embed_dim=embedding_size, \n",
        "                                             num_heads=num_heads, \n",
        "                                             dropout= dropout, \n",
        "                                             bias=False)\n",
        "\n",
        "# [BATCH_SIZE, SEQUENCE_LENGTH, EMBED_SIZE]\n",
        "src = torch.randn(batch_size, sequence_length, embedding_size)\n",
        "src = src.permute(1, 0, 2)\n",
        "\n",
        "# query = key = value for self_attention\n",
        "attn_output, attn_output_weights = self_attention(src, src, src)\n",
        "attn_output.size(), attn_output_weights.size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([10, 4, 6]), torch.Size([4, 10, 10]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaYsjqv2gpem",
        "colab_type": "code",
        "outputId": "5f3fab19-f68d-4e23-a262-3a1892fbef3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# src.size() = [BATCH_SIZE, SEQENCE_LEN, EMBEDDING_DIM]\n",
        "# change to [SEQUENCE_LEN, BATCH_SIZE, EMBEDDING_DIM]\n",
        "src.size(), src.permute(1, 0, 2).size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([10, 4, 6]), torch.Size([4, 10, 6]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqyD2PzGhbHu",
        "colab_type": "code",
        "outputId": "c2c3f6b5-7d0b-4a07-c220-329701d19af4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Encoder and Decoder Stack\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# - Encoder layer has two sub-layers \n",
        "#       - first : Multihead Attention Layer\n",
        "#       - second : fc layer\n",
        "# - After each sublayer we use fc sub layer\n",
        "\n",
        "'''\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 multi_head_attn_layer,\n",
        "                 feed_forward_layer,\n",
        "                 num_embeddings,\n",
        "                 embedding_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.multi_head_attn_layer = multi_head_attn_layer\n",
        "        self.feed_forward_layer = feed_forward_layer\n",
        "\n",
        "        self.embed = nn.Embedding(num_embeddings, embedding_dim)\n",
        "\n",
        "\n",
        "    def layer_norm(self, out):\n",
        "        layer_norm = nn.LayerNorm(out.size()[1:])\n",
        "        return layer_norm\n",
        "\n",
        "\n",
        "    def forward(self, src):\n",
        "        # TODO: Add Residual Connections\n",
        "        # TODO: Fix the sizes of the input/output tensors\n",
        "        # TODO: Add layer Norm\n",
        "\n",
        "        embed = self.embed(src)\n",
        "        attn_output, _ = self.multi_head_attn_layer(embed, embed, embed)\n",
        "        out = torch.sum(embed, attn_output)\n",
        "        layer_norm = nn.LayerNorm(out.size()[1:])\n",
        "\n",
        "        out = layer_norm(out)\n",
        "        out = self.feed_forward_layer(out)\n",
        "        out = layer_norm(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class MultiheadAttentionLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 embed_dim,\n",
        "                 num_heads,\n",
        "                 dropout,):\n",
        "        super(MultiheadAttentionLayer, self).__init__()\n",
        "\n",
        "        self.multi_head_attn_layer = nn.MultiheadAttention(embed_dim=embed_dim,\n",
        "                                                           num_heads=num_heads,\n",
        "                                                           dropout=dropout,\n",
        "                                                           bias=False)\n",
        "    def forward(self, src):\n",
        "        attn_output, _ = self.multi_head_attn_layer(src, src, src)\n",
        "        return attn_output\n",
        "\n",
        "\n",
        "class FeedForwardLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_size,\n",
        "                 output_size,\n",
        "                 dropout):\n",
        "        supper(FeedforwarLayer, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(in_features=self.input_size,\n",
        "                            out_features=self.output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.dropout(nn.ReLU(self.fc(x)))\n",
        "        return out\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \n",
        "    # In addition to the two layers present in the encoder the decoder has one\n",
        "    # more multihead attention layer that performs attention over the output of\n",
        "    # the encoder stack.\n",
        "    \n",
        "    def __init__(self, \n",
        "                 multihead_attn_layer, \n",
        "                 feed_forward_layer,\n",
        "                 dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.multihead_attn_layer = multihead_attn_layer\n",
        "        self.feed_forward_layer = feed_forward_layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self):\n",
        "        return\n",
        "\n",
        "'''\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr8HB9F2mUQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 multi_head_attn_layer,\n",
        "                 feed_forward_layer,\n",
        "                 num_embeddings,\n",
        "                 embedding_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.multi_head_attn_layer = multi_head_attn_layer\n",
        "        self.feed_forward_layer = feed_forward_layer\n",
        "\n",
        "        self.embed = nn.Embedding(num_embeddings, embedding_dim)\n",
        "\n",
        "\n",
        "    def layer_norm(self, out):\n",
        "        layer_norm = nn.LayerNorm(out.size()[1:])\n",
        "        return layer_norm\n",
        "\n",
        "\n",
        "    def forward(self, src):\n",
        "        # TODO: Add Residual Connections\n",
        "        # TODO: Fix the sizes of the input/output tensors\n",
        "        # TODO: Add layer Norm\n",
        "\n",
        "        embed = self.embed(src)\n",
        "        attn_output, _ = self.multi_head_attn_layer(embed, embed, embed)\n",
        "        out = torch.sum(embed, attn_output)\n",
        "        layer_norm = nn.LayerNorm(out.size()[1:])\n",
        "\n",
        "        out = layer_norm(out)\n",
        "        out = self.feed_forward_layer(out)\n",
        "        out = layer_norm(out)\n",
        "        return out\n",
        "\n",
        "#multi_head_attn_layer = MultiheadAttentionLayer(embed_dim=12, num_heads=3, dropout=0.5)\n",
        "#feed_forward_layer = FeedForwardLayer()\n",
        "#encoder = Encoder(multi_head_attn_layer=multi_head_attn_layer, feed_forward_layer=feed_forward_layer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPYnTMVvAQml",
        "colab_type": "code",
        "outputId": "fc8e5ebd-7442-4837-e8bd-34cbb772fab5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from time import time, perf_counter\n",
        "\n",
        "start = perf_counter()\n",
        "\n",
        "\n",
        "class MultiheadAttentionLayer(nn.Module):\n",
        "    '''\n",
        "    Takes in embeddings and returns attention tensors/output\n",
        "    '''\n",
        "    def __init__(self, embed_dim, num_heads, dropout):\n",
        "        super(MultiheadAttentionLayer, self).__init__()\n",
        "        self.multi_head_attn_layer = nn.MultiheadAttention(embed_dim=embed_dim,\n",
        "                                                           num_heads=num_heads,\n",
        "                                                           dropout=dropout,\n",
        "                                                           bias=True)\n",
        "    def forward(self, src):\n",
        "        attn_output, _ = self.multi_head_attn_layer(src, src, src)\n",
        "        return attn_output\n",
        "\n",
        "# Create the MultiheadAttentionLayer object\n",
        "multihead = MultiheadAttentionLayer(embed_dim=512, num_heads=8, dropout=0.0)\n",
        "\n",
        "\n",
        "# REMEMBER: Add padding to each sentences in the batch to have the same size\n",
        "seq_len = 12\n",
        "bs = 4\n",
        "edim = 512\n",
        "# A batch of equal sized sentences\n",
        "#src = torch.randn(seq_len, bs, edim)\n",
        "src = torch.randn(4, 512).unsqueeze(0)\n",
        "\n",
        "# Output of multihead attention layer\n",
        "attn_output = multihead(src)\n",
        "\n",
        "print(attn_output.size())\n",
        "\n",
        "# For calculating time\n",
        "end = perf_counter()\n",
        "time_taken = end - start\n",
        "print(time_taken)\n",
        "\n",
        "''' COMMENT: MultiheadAttentionLayer sublayer works. '''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 4, 512])\n",
            "0.012960660999993934\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' COMMENT: MultiheadAttentionLayer sublayer works. '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRIB4Lq2vyCJ",
        "colab_type": "code",
        "outputId": "f38f5530-9143-452f-bfae-89489da407b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "fc = nn.Linear(16, 32)\n",
        "fc(torch.randn(4, 2, 16)).size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 2, 32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwShL8znpD1Q",
        "colab_type": "code",
        "outputId": "afac2e7c-47aa-4408-c596-dec0b5c9bbd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "'''\n",
        "    Here the focus is input the output of multi-head attention layer to \n",
        "    LayerNorm and then Feed Forward Network\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n    Here the focus is input the output of multi-head attention layer to \\n    LayerNorm and then Feed Forward Network\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORCMMtJdnao3",
        "colab_type": "code",
        "outputId": "5f564439-8cc1-472e-d494-661e997890cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(attn_output.size())\n",
        "\n",
        "attn_output = attn_output.permute(1, 0, 2)\n",
        "print(attn_output.reshape(4,-1).size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 4, 512])\n",
            "torch.Size([4, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDG4gVG1qRfl",
        "colab_type": "code",
        "outputId": "0bc14ee9-4f94-4851-c494-120fed3f7758",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "norm = nn.LayerNorm(512)\n",
        "lnorm = norm(attn_output)\n",
        "lnorm.size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 1, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSH-r3rGjgAK",
        "colab_type": "code",
        "outputId": "51ca2fac-43e4-4b29-8ff7-c0f0d3930349",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "class FeedForwardLayer(nn.Module):\n",
        "    def __init__(self, input_size, dropout):\n",
        "        super(FeedForwardLayer, self).__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features=input_size, out_features=2048)\n",
        "        \n",
        "        self.fc2 = nn.Linear(in_features=2048, out_features=512)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = input.view(input.size(0), -1)\n",
        "        \n",
        "        out = self.dropout(self.relu(self.fc1(input)))\n",
        "        out = self.dropout(self.relu(self.fc2(out)))\n",
        "        print(f\"{'-'*10} SUCCESS {'-'*10}\\n\")\n",
        "        return out\n",
        "\n",
        "\n",
        "feed_forward_layer = FeedForwardLayer(input_size=512, dropout=0.5)\n",
        "\n",
        "fc_out = feed_forward_layer(torch.randn(4, 12, 512))\n",
        "print(fc_out.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------- SUCCESS ----------\n",
            "\n",
            "torch.Size([4, 12, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiW1IhJ09bY2",
        "colab_type": "code",
        "outputId": "b925c093-c816-4781-dfc1-949b60ab5d3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.attn_layer = MultiheadAttentionLayer(embed_dim,\n",
        "                                                  num_heads,\n",
        "                                                  dropout)\n",
        "        \n",
        "        self.feed_forward_layer = FeedForwardLayer(embed_dim, dropout)\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "\n",
        "    def forward(self, src):\n",
        "        # multihead attention sub-layer\n",
        "        attn_output = self.attn_layer(src)  \n",
        "\n",
        "        # add and norm      \n",
        "        attn = attn_output.add(src)\n",
        "        _attn = self.norm(attn)\n",
        "\n",
        "        # feed forward layer\n",
        "        out = self.feed_forward_layer(_attn)\n",
        "\n",
        "        # add and norm\n",
        "        out = out.add(_attn)\n",
        "        out = self.norm(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "encoder_layer = EncoderLayer(512, 8, 0.5)\n",
        "\n",
        "encoder_layer(torch.randn(4, 12, 512)).size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------- SUCCESS ----------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 12, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZH6cWoxHxvaB",
        "colab_type": "code",
        "outputId": "8a4e89cc-7960-4d83-ca81-0c0a2f53f873",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout, max_len):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.encoder = EncoderLayer(embed_dim, num_heads, dropout)\n",
        "\n",
        "        self.embed = nn.Embedding(max_len, embed_dim)\n",
        "        self.pos = nn.Embedding(max_len, embed_dim)\n",
        "\n",
        "    def forward(self, src):\n",
        "        _embed = self.embed(src)\n",
        "        _pos = self.pos(src)\n",
        "        _src = _embed.add(_pos)\n",
        "        out = self.encoder(_src)\n",
        "        return out\n",
        "\n",
        "_encoder = Encoder(512, 8, 0.5, 12)\n",
        "\n",
        "# A batch of sequences/tokens or sentences\n",
        "_out = _encoder(torch.ones((4, 12), dtype=torch.long))\n",
        "_out.size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------- SUCCESS ----------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 12, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gFlUfD-LVdx",
        "colab_type": "code",
        "outputId": "9dafa67a-6ff3-420b-9c4b-7169da62116f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        \n",
        "        \n",
        "        self.self_attn = MultiheadAttentionLayer(embed_dim, \n",
        "                                                 num_heads, \n",
        "                                                 dropout)\n",
        "        \n",
        "        self.feed_forward = FeedForwardLayer(embed_dim, dropout)\n",
        "\n",
        "        self.encoder_attn = MultiheadAttentionLayer(embed_dim, \n",
        "                                                    num_heads, \n",
        "                                                    dropout)\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, trg, positional_encoding):\n",
        "        # TODO: How the mask works in this case.\n",
        "\n",
        "        \n",
        "        # masked multihead attention\n",
        "        attn_output = self.self_attn(trg)\n",
        "\n",
        "        # add and norm\n",
        "        attn_output = attn_output.add(positional_encoding)\n",
        "        norm_output = self.norm(attn_output)\n",
        "\n",
        "        # encoder multihead attention\n",
        "        output = self.encoder_attn(norm_output)\n",
        "\n",
        "        # add and norm\n",
        "        output = output.add(norm_output)\n",
        "\n",
        "        # feed forwarfd layer\n",
        "        fc_out = self.feed_forward(output)\n",
        "\n",
        "        # add and norm\n",
        "        out = fc_out.add(output)\n",
        "        out = self.norm(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "dlayer = DecoderLayer(512, 8, 0.5)\n",
        "trg = torch.randn(4, 12, 512)\n",
        "dlayer(trg, trg).size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------- SUCCESS ----------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 12, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjrSW8b7RjKQ",
        "colab_type": "code",
        "outputId": "d259250a-8dcd-408d-fe3f-79187249ab23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout, max_len):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.decoder = DecoderLayer(embed_dim, num_heads, dropout)\n",
        "        self.embed = nn.Embedding(max_len, embed_dim)\n",
        "        self.pos = nn.Embedding(max_len, embed_dim)\n",
        "        self.fc = nn.Linear(embed_dim, max_len)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "    def forward(self, trg):\n",
        "        _trg = self.embed(trg)\n",
        "        _pos = self.embed(trg)\n",
        "        _trg = _trg + _pos\n",
        "        attn_out = self.decoder(_trg, _pos)\n",
        "        out = self.softmax(self.fc(attn_out))\n",
        "        return out\n",
        "\n",
        "_decoder_ = Decoder(512, 8,  0.5, 33)\n",
        "_decoder_(torch.ones((32, 33), dtype=torch.long)).size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------- SUCCESS ----------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 33, 33])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7skV-0F_gDPx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Transformer, self).__init__()\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umFpAqbTGYar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.attn_layer = MultiheadAttentionLayer(embed_dim,\n",
        "                                                  num_heads,\n",
        "                                                  dropout)\n",
        "        \n",
        "        self.feed_forward_layer = FeedForwardLayer(embed_dim, dropout)\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "\n",
        "    def forward(self, src):\n",
        "        # multihead attention sub-layer\n",
        "        attn_output = self.attn_layer(src)  \n",
        "\n",
        "        # add and norm      \n",
        "        attn = attn_output.add(src)\n",
        "        _attn = self.norm(attn)\n",
        "\n",
        "        # feed forward layer\n",
        "        out = self.feed_forward_layer(_attn)\n",
        "\n",
        "        # add and norm\n",
        "        out = out.add(_attn)\n",
        "        out = self.norm(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.encoder = EncoderLayer(embed_dim, num_heads, dropout)\n",
        "\n",
        "        self.embed = nn.Embedding(12, embed_dim)\n",
        "        self.pos = nn.Embedding(12, embed_dim)\n",
        "\n",
        "    def forward(self, src):\n",
        "        _embed = self.embed(src)\n",
        "        _pos = self.pos(src)\n",
        "        _src = _embed.add(_pos)\n",
        "        out = self.encoder(_src)\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedForwardLayer(nn.Module):\n",
        "    def __init__(self, input_size, dropout):\n",
        "        super(FeedForwardLayer, self).__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features=input_size, out_features=2048)\n",
        "        \n",
        "        self.fc2 = nn.Linear(in_features=2048, out_features=512)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = input.view(input.size(0), -1)\n",
        "        \n",
        "        out = self.dropout(self.relu(self.fc1(input)))\n",
        "        out = self.dropout(self.relu(self.fc2(out)))\n",
        "        print(f\"{'-'*10} SUCCESS {'-'*10}\\n\")\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiheadAttentionLayer(nn.Module):\n",
        "    '''\n",
        "    Takes in embeddings and returns attention tensors/output\n",
        "    '''\n",
        "    def __init__(self, embed_dim, num_heads, dropout):\n",
        "        super(MultiheadAttentionLayer, self).__init__()\n",
        "        self.multi_head_attn_layer = nn.MultiheadAttention(embed_dim=embed_dim,\n",
        "                                                           num_heads=num_heads,\n",
        "                                                           dropout=dropout,\n",
        "                                                           bias=True)\n",
        "    def forward(self, src):\n",
        "        attn_output, _ = self.multi_head_attn_layer(src, src, src)\n",
        "        return attn_output\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, fc_out, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        \n",
        "        \n",
        "        self.self_attn = MultiheadAttentionLayer(embed_dim, \n",
        "                                                 num_heads, \n",
        "                                                 dropout)\n",
        "        \n",
        "        self.feed_forward = FeedForwardLayer(embed_dim, dropout)\n",
        "\n",
        "        self.encoder_attn = MultiheadAttentionLayer(embed_dim, \n",
        "                                                    num_heads, \n",
        "                                                    dropout)\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, trg, trg_mask, positional_encoding):\n",
        "        # TODO: How the mask works in this case.\n",
        "\n",
        "        \n",
        "        # masked multihead attention\n",
        "        attn_output = self.self_attn(trg)\n",
        "\n",
        "        # add and norm\n",
        "        attn_output = attn_output.add(positional_encoding)\n",
        "        norm_output = self.norm(attn_output)\n",
        "\n",
        "        # encoder multihead attention\n",
        "        output = self.encoder_attn(norm_output)\n",
        "\n",
        "        # add and norm\n",
        "        output = output.add(norm_output)\n",
        "\n",
        "        # feed forwarfd layer\n",
        "        fc_out = self.feed_forward(output)\n",
        "\n",
        "        # add and norm\n",
        "        out = fc_out.add(output)\n",
        "        out = self.norm(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout, max_len):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.decoder = DecoderLayer(embed_dim, num_heads, dropout)\n",
        "        self.embed = nn.Embedding(max_len, embed_dim)\n",
        "        self.pos = nn.Embedding(max_len, embed_dim)\n",
        "        self.fc = nn.Linear(embed_dim, max_len)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "    def forward(self, trg):\n",
        "        _trg = self.embed(trg)\n",
        "        _pos = self.embed(trg)\n",
        "        _trg = _trg + _pos\n",
        "        attn_out = self.decoder(_trg, _pos)\n",
        "        out = self.softmax(self.fc(attn_out))\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}